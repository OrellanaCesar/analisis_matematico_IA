{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ce50a8",
   "metadata": {},
   "source": [
    "## RESPUESTAS \n",
    "#### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
    "\n",
    "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
    "\n",
    "Sobre las $k$ clases.\n",
    "\n",
    "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
    "\n",
    "`tensor_inv_covs` es un array de NumPy de $\\mathbb{R}^{k \\times p \\times p}$\n",
    "\n",
    "`tensor_means` es un array de NumPy de $\\mathbb{R}^{k \\times p \\times 1}$\n",
    "\n",
    "QDA en el método `_predict_one` itera sobre las $k$ clases calculando el logaritmo de la probabilidad a posteriori para cada una, usando el método `_predict_log_conditional` (este método devuelve un escalar), obteniendo una lista (de tamaño $k$) con la probabilidad de que la observación pertenezca a cada una de las clases. Luego, devuelve el argumento de la mayor probabilidad en dicha lista.\n",
    "\n",
    "- inv_cov $\\in \\mathbb{R}^{p \\times p}$\n",
    "- means, unbiased_x $\\in \\mathbb{R}^{p \\times 1}$\n",
    "- unbiased_x.T @ inv_cov @ unbiased_x $\\rarr$ $(1 \\times p) @ (p \\times p) @ (p \\times 1) $ $\\in \\mathbb{R}^{1 \\times 1} \\rarr $ escalar\n",
    "\n",
    "Entonces, `_predict_log_conditional` devuelve un escalar.\n",
    "\n",
    "TensorizedQDA directamente hace el cálculo de los logaritmos de las probabilidades a posteriori de forma matricial en el método `_predict_log_conditionals`, el cual directamente devuelve un vector columna $\\mathbb{R}^{k \\times 1}$, similar a la lista obtenida con el bucle for dentro del método `_predict_one` de la clase `QDA`. Luego, el método `_predict_one` para esta clase solo se encarga de encotrar el argumento que maximiza la probabilidad (logarítmica).\n",
    "\n",
    "- tensor_inv_cov $\\in \\mathbb{R}^{k \\times p \\times p}$\n",
    "- tensor_means, unbiased_x  $\\in \\mathbb{R}^{k \\times p \\times 1}$\n",
    "- inner_prod $\\in \\mathbb{R}^{k \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7653d5",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71311f41",
   "metadata": {},
   "source": [
    "### 2) Optimización\n",
    "\n",
    "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
    "\n",
    "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246eaf3",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "Poder eliminar el ciclo for de predict. Es decir, predecir las n observaciones y k clases en un mismo paso. \n",
    "\n",
    "En BaseBayesianClassifier\n",
    "\n",
    "predict(self, X) → tiene ciclo for recorre las n observaciones, y llama a predict_one, donde TensorizedQDA ya paraleliza las k clases. \n",
    "\n",
    "-------------\n",
    "def predict(self, X): → llama a predict_one por cada observación\n",
    "\n",
    "def _predict_one(self, x): → llama a _predict_log_conditional para hacer el argmax de la suma\n",
    "\n",
    "$$\n",
    "\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j\n",
    "$$\n",
    "\n",
    "def _predict_log_conditional(self, x, class_idx)\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "----------------\n",
    "\n",
    "Buscamos calcular la forma cuadrática para muchas observaciones a la vez:\n",
    "\n",
    "$$(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$$\n",
    "\n",
    "Donde usamos : _predict_log_conditional(x, class_idx)\n",
    "\n",
    "unbiased_x = x - mean_j\n",
    "\n",
    "unbiased_x.T @ inv_cov_j @ unbiased_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0bc62",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2e0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy        as np\n",
    "import pandas       as pd\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg           import cholesky, solve_triangular\n",
    "from scipy.linalg.lapack    import dtrtri\n",
    "\n",
    "from base.qda               import QDA, TensorizedQDA\n",
    "from base.cholesky          import QDA_Chol1, QDA_Chol2, QDA_Chol3\n",
    "from utils.bench            import Benchmark\n",
    "from utils.datasets         import (get_letters_dataset,label_encode)                                                     \n",
    "from numpy.random           import RandomState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14291914",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50995c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benching params:\n",
      "Total runs: 150\n",
      "Warmup runs: 20\n",
      "Peak Memory usage runs: 30\n",
      "Running time runs: 100\n",
      "Train size rows (approx): 16000\n",
      "Test size rows (approx): 4000\n",
      "Test size fraction: 0.2\n"
     ]
    }
   ],
   "source": [
    "# dataset de letters\n",
    "X_letter, y_letter = get_letters_dataset()\n",
    "\n",
    "# encoding de labels\n",
    "y_letter_encoded = label_encode(y_letter.reshape(-1,1)) # hago reshape para que quede como matriz columna\n",
    "\n",
    "# instanciacion del benchmark\n",
    "b = Benchmark(\n",
    "    X_letter, y_letter_encoded,\n",
    "    same_splits=False,\n",
    "    n_runs=100,\n",
    "    warmup=20,\n",
    "    mem_runs=30,\n",
    "    test_sz=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810f7778",
   "metadata": {},
   "source": [
    "### Prueba QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "735a04a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  5, 18,  7,  7]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QDA()\n",
    "\n",
    "qda.fit(X_letter.T, y_letter_encoded)\n",
    "\n",
    "qda.predict(X_letter.T[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24cec0",
   "metadata": {},
   "source": [
    "### Prueba TensorizedQDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b54275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  5, 18,  7,  7]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqda = TensorizedQDA()\n",
    "\n",
    "tqda.fit(X_letter.T, y_letter_encoded)\n",
    "\n",
    "tqda.predict(X_letter.T[:, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f1a89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701b160447744b7dbb4a721134b8077a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58031a688ccb4cb0976a7fddb9a7512f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fcc4ff0aab40d483b695a49f626e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedQDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb15f2d985f4438ea25210678624b585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedQDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QDA \n",
    "b.bench(QDA)\n",
    "\n",
    "# TensorizedQDA\n",
    "b.bench(TensorizedQDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634477c4",
   "metadata": {},
   "source": [
    "### FasterQDA\n",
    "\n",
    "Definir una clase FasterQDA que herede de TensorizedQDA y redefina predict para predecir todas las observaciones juntas, sin el for sobre filas de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b21da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    \"\"\"\n",
    "    Versión vectorizada (sin bucles en predict), \n",
    "    con matriz intermedia ineficiente (matriz N x N).\n",
    "    \"\"\"\n",
    "\n",
    "    def _predict_log_conditionals_batch(self, X):\n",
    "        # Dimensiones iniciales\n",
    "        k, p, _ = self.tensor_means.shape # self.tensor_means: (k, p, 1)    → k clases, p features\n",
    "        n = X.shape[1]                    # X: (p, n)                       → p features, n observaciones\n",
    "\n",
    "        # Cálculo resta: D = X - medias\n",
    "        D = X[None, :, :] - self.tensor_means   # X: (1, p, n) - Medias: (k, p, 1) → D: (k, p, n)\n",
    "\n",
    "        # Cálculo de la distancia cuadrática \n",
    "        quad_terms = np.empty((k, n)) # Almacena distancias por cada k clase y observación n\n",
    "\n",
    "        for j in range(k):\n",
    "            D_j = D[j]                          # Shape: (p, n)\n",
    "            inv_cov_j = self.tensor_inv_cov[j]  # Shape: (p, p)\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # Generación de matriz n x n\n",
    "            # ---------------------------------------------------------\n",
    "            # Al multiplicar D_j.T (n, p) @ inv (p, p) @ D_j (p, n), el resultado final es una matriz de (n, n)\n",
    "            # solo nos importa cada dato consigo mismo (la diagonal)\n",
    "            \n",
    "            Q_j = D_j.T @ inv_cov_j @ D_j      # matriz (n, n)\n",
    "\n",
    "            # Nos quedamos con la diagonal \n",
    "            quad_terms[j, :] = np.diag(Q_j)    \n",
    "            # ---------------------------------------------------------\n",
    "\n",
    "        \n",
    "        # log_det es (k,). Lo convertimos a (k, 1) para operar con quad_terms\n",
    "        log_det = np.log(LA.det(self.tensor_inv_cov))[:, None]\n",
    "        \n",
    "        # Fórmula final: 0.5 * log_det - 0.5 * distancia\n",
    "        return 0.5 * log_det - 0.5 * quad_terms\n",
    "\n",
    "    def predict(self, X):\n",
    "        # traigo el log-condicionales \n",
    "        log_cond = self._predict_log_conditionals_batch(X)\n",
    "\n",
    "        # Sumamos el log_a_priori \n",
    "        log_post = self.log_a_priori[:, None] + log_cond\n",
    "\n",
    "        # Elegimos la clase ganadora → por columna\n",
    "        y_hat = np.argmax(log_post, axis=0)\n",
    "\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e082abd",
   "metadata": {},
   "source": [
    "----\n",
    "5. Demostrar que\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$\n",
    "es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
    "$$\n",
    "np.sum(A^T \\odot B, axis=0).T\n",
    "$$\n",
    "queda a preferencia del alumno cuál usar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb15db",
   "metadata": {},
   "source": [
    "### Demostración paso a paso\n",
    "\n",
    "El elemento $(i,j)$ de $A \\cdot B$ se calcula como:\n",
    "$$\n",
    "(A \\cdot B)_{ij} = \\sum_{k=1}^{p} A_{ik} \\cdot B_{kj}\n",
    "$$\n",
    "\n",
    "Es decir, el **producto interno** entre la fila $i$ de $A$ y la columna $j$ de $B$.\n",
    "\n",
    "Para la diagonal, solo nos interesan los elementos donde $i = j$:\n",
    "$$\n",
    "(A \\cdot B)_{ii} = \\sum_{k=1}^{p} A_{ik} \\cdot B_{ki}\n",
    "$$\n",
    "\n",
    "Se observa que $B_{ki}$ es el elemento en la posición $(k, i)$ de $B$, que es exactamente el elemento $(i, k)$ de $B^T$.\n",
    "\n",
    "Por lo tanto:\n",
    "$$\n",
    "(A \\cdot B)_{ii} = \\sum_{k=1}^{p} A_{ik} \\cdot (B^T)_{ik}\n",
    "$$\n",
    "\n",
    "\n",
    "Si hacemos el producto elemento a elemento (Hadamard) $A \\odot B^T$ :\n",
    "$$\n",
    "(A \\odot B^T)_{ik} = A_{ik} \\cdot (B^T)_{ik}\n",
    "$$\n",
    "\n",
    "Y luego sumamos a lo largo de las columnas (axis=1):\n",
    "$$\n",
    "\\sum_{k=1}^{p} (A \\odot B^T)_{ik} = \\sum_{k=1}^{p}  A_{ik} \\cdot (B^T)_{ik}    = (A \\cdot B)_{ii}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97df2d5",
   "metadata": {},
   "source": [
    "### EJEMPLO matriz 2x2\n",
    "\n",
    "Haciendo el producto matricial entre $A$ y $B$:\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{pmatrix}\n",
    "\\quad\n",
    "B = \\begin{pmatrix}\n",
    "e & f \\\\\n",
    "g & h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A \\cdot B = \\begin{pmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "e & f \\\\\n",
    "g & h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A \\cdot B = \\begin{pmatrix}\n",
    "ae + bg & af + bh \\\\\n",
    "ce + dg & cf + dh\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{diag}(A \\cdot B) = \\begin{pmatrix}\n",
    "ae + bg \\\\\n",
    "cf + dh\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Haciendo el producto elemento a elemento entre $A$ y $B^T$ y sumando:\n",
    "$$\n",
    "B^T = \\begin{pmatrix}\n",
    "e & g \\\\\n",
    "f & h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A \\odot B^T = \\begin{pmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{pmatrix}\n",
    "\\odot\n",
    "\\begin{pmatrix}\n",
    "e & g \\\\\n",
    "f & h\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "a \\cdot e & b \\cdot g \\\\\n",
    "c \\cdot f & d \\cdot h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{np.sum}(A \\odot B^T, \\text{axis}=1) = \\begin{pmatrix}\n",
    "ae + bg \\\\\n",
    "cf + dh\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\text{diag}(A \\cdot B) = \\begin{pmatrix}\n",
    "ae + bg \\\\\n",
    "cf + dh\n",
    "\\end{pmatrix}\n",
    "= \\text{np.sum}(A \\odot B^T, \\text{axis}=1)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b9198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal original:\n",
      " [ 0.85766051 -2.13315315 -0.50185242  1.93709816 -2.48132112]\n",
      "Suma optimizada:\n",
      " [ 0.85766051 -2.13315315 -0.50185242  1.93709816 -2.48132112]\n",
      "¿Son iguales? True\n"
     ]
    }
   ],
   "source": [
    "# Demostración en código:\n",
    "\n",
    "# Se definen dimensiones arbitrarias\n",
    "n = 5\n",
    "k = 7\n",
    "\n",
    "# Se crean matrices aleatorias, con esas dimensiones\n",
    "A = np.random.randn(n, k)\n",
    "B = np.random.randn(k, n)\n",
    "\n",
    "# Forma costosa, con producto matricial completo y extrayendo diagonal\n",
    "producto_matricial = A @ B  # Dimensión (n, n)\n",
    "diagonal_original = np.diag(producto_matricial)\n",
    "\n",
    "# Forma optimizada, con las sumas del producto elemento a elemento\n",
    "# Se transpone B: (k, n) -> (n, k), para que coincida con A\n",
    "optimizada = np.sum(A * B.T, axis=1)\n",
    "\n",
    "print(\"Diagonal original:\\n\", diagonal_original)\n",
    "print(\"Suma optimizada:\\n\", optimizada)\n",
    "print(\"Son iguales?\",np.allclose(diagonal_original, optimizada)) Se usa allclose por si hay errores de redondeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40832a2",
   "metadata": {},
   "source": [
    "----\n",
    "6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d24e5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientQDA(TensorizedQDA):\n",
    "    \"\"\"\n",
    "    Versión que evita la matriz nxn\n",
    "    usando el producto de Hadamard para calcular solo la diagonal.\n",
    "    \"\"\"\n",
    "\n",
    "    def _predict_log_conditionals_batch(self, X):\n",
    "        # Dimensiones iniciales\n",
    "        k, p, _ = self.tensor_means.shape  # self.tensor_means: (k, p, 1) → k clases, p features\n",
    "        n = X.shape[1]                     # X: (p, n)                    → p features, n observaciones\n",
    "\n",
    "        # Cálculo resta: D = X - medias\n",
    "        D = X[None, :, :] - self.tensor_means   # X: (1, p, n) - Medias: (k, p, 1) → D: (k, p, n)\n",
    "\n",
    "        # Cálculo de la distancia cuadrática \n",
    "        quad_terms = np.empty((k, n))  # Almacena distancias por cada k clase y observación n\n",
    "\n",
    "        for j in range(k):\n",
    "            D_j = D[j]                          # Shape: (p, n)\n",
    "            inv_cov_j = self.tensor_inv_cov[j]  # Shape: (p, p)\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # MÉTODO EFICIENTE: usando producto de Hadamard\n",
    "            # ---------------------------------------------------------\n",
    "            # En vez de hacerr Q_j = D_j.T @ inv_cov_j @ D_j  → (n, n)\n",
    "            # Calculamos lo siguiente diag(A @ B) = sum(A ⊙ B.T, axis=1)\n",
    "            \n",
    "            # Es inevitable tener que calcular alguno de los productos matriciales al haber 3 matrices en la multiplicación.\n",
    "            # 1) A = D_j.T @ inv_cov_j  → (n, p) \n",
    "            A = D_j.T @ inv_cov_j\n",
    "            \n",
    "            # 2) B = D_j  → (p, n), entonces B.T = D_j.T → (n, p)\n",
    "            # 3) diag(A @ D_j) = sum(A ⊙ D_j.T, axis=1)\n",
    "            quad_terms[j, :] = np.sum(A * D_j.T, axis=1)\n",
    "            # ---------------------------------------------------------\n",
    "\n",
    "        # log_det es (k,). Lo convertimos a (k, 1) para operar con quad_terms\n",
    "        log_det = np.log(LA.det(self.tensor_inv_cov))[:, None]\n",
    "        \n",
    "        # Fórmula final: 0.5 * log_det - 0.5 * distancia\n",
    "        return 0.5 * log_det - 0.5 * quad_terms\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Traigo los log-condicionales \n",
    "        log_cond = self._predict_log_conditionals_batch(X)\n",
    "\n",
    "        # Sumamos el log_a_priori \n",
    "        log_post = self.log_a_priori[:, None] + log_cond\n",
    "\n",
    "        # Elegimos la clase ganadora → por columna\n",
    "        y_hat = np.argmax(log_post, axis=0)\n",
    "\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553097f6",
   "metadata": {},
   "source": [
    "----\n",
    "7. Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2cf8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c08f2c444b7497fa90bc208ade2ca2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FasterQDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FasterQDA\n",
    "b.bench(FasterQDA)\n",
    "\n",
    "# EfficientQDA\n",
    "b.bench(EfficientQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119f3fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_median_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_median_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_median_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_median_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>8.16250</td>\n",
       "      <td>7.412496</td>\n",
       "      <td>1729.55460</td>\n",
       "      <td>1612.031217</td>\n",
       "      <td>0.886117</td>\n",
       "      <td>0.270096</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.099219</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>7.26470</td>\n",
       "      <td>0.616768</td>\n",
       "      <td>289.93255</td>\n",
       "      <td>3.242220</td>\n",
       "      <td>0.885303</td>\n",
       "      <td>0.268463</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.154099</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>1.123584</td>\n",
       "      <td>5.965369</td>\n",
       "      <td>1.006082</td>\n",
       "      <td>0.643866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>7.49610</td>\n",
       "      <td>1.225837</td>\n",
       "      <td>1095.82545</td>\n",
       "      <td>34.262841</td>\n",
       "      <td>0.884827</td>\n",
       "      <td>0.268951</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>258.234673</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>1.088900</td>\n",
       "      <td>1.578312</td>\n",
       "      <td>1.004255</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>7.14195</td>\n",
       "      <td>0.627668</td>\n",
       "      <td>11.95610</td>\n",
       "      <td>0.908321</td>\n",
       "      <td>0.884890</td>\n",
       "      <td>0.269440</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>15.743172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142895</td>\n",
       "      <td>144.658760</td>\n",
       "      <td>1.002435</td>\n",
       "      <td>0.006302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA_2</th>\n",
       "      <td>7.14635</td>\n",
       "      <td>1.065439</td>\n",
       "      <td>16.78935</td>\n",
       "      <td>1.380324</td>\n",
       "      <td>0.884770</td>\n",
       "      <td>0.268402</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>15.254768</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.142191</td>\n",
       "      <td>103.014983</td>\n",
       "      <td>1.006310</td>\n",
       "      <td>0.006504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
       "model                                                                        \n",
       "QDA                     8.16250      7.412496      1729.55460  1612.031217   \n",
       "TensorizedQDA           7.26470      0.616768       289.93255     3.242220   \n",
       "FasterQDA               7.49610      1.225837      1095.82545    34.262841   \n",
       "EfficientQDA            7.14195      0.627668        11.95610     0.908321   \n",
       "EfficientQDA_2          7.14635      1.065439        16.78935     1.380324   \n",
       "\n",
       "                mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
       "model                                                                  \n",
       "QDA                  0.886117             0.270096          0.002008   \n",
       "TensorizedQDA        0.885303             0.268463          0.002143   \n",
       "FasterQDA            0.884827             0.268951          0.001919   \n",
       "EfficientQDA         0.884890             0.269440          0.002171   \n",
       "EfficientQDA_2       0.884770             0.268402          0.002159   \n",
       "\n",
       "                test_mem_median_mb  test_mem_std_mb  train_speedup  \\\n",
       "model                                                                \n",
       "QDA                       0.099219         0.001183       1.000000   \n",
       "TensorizedQDA             0.154099         0.000018       1.123584   \n",
       "FasterQDA               258.234673         0.000541       1.088900   \n",
       "EfficientQDA             15.743172         0.000000       1.142895   \n",
       "EfficientQDA_2           15.254768         0.000008       1.142191   \n",
       "\n",
       "                test_speedup  train_mem_reduction  test_mem_reduction  \n",
       "model                                                                  \n",
       "QDA                 1.000000             1.000000            1.000000  \n",
       "TensorizedQDA       5.965369             1.006082            0.643866  \n",
       "FasterQDA           1.578312             1.004255            0.000384  \n",
       "EfficientQDA      144.658760             1.002435            0.006302  \n",
       "EfficientQDA_2    103.014983             1.006310            0.006504  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = b.summary(baseline=\"QDA\")\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de007667",
   "metadata": {},
   "source": [
    "Vemos que TensorizedQDA entrena en el mismo tiempo que el QDA base y clasifica con la misma accuracy, pero es mucho más rápido en predicción.\n",
    "FasterQDA no lo mejora porque empieza a comparar todas las observaciones contra todas y arma una matriz gigante N×N que consume 258 MB de RAM y ralentiza todo el proceso.\n",
    "\n",
    "Se puede ver que EfficientQDA es el más rápido de todos para predecir y reduce enormemente el consumo de memoria respecto al FasterQDA, debido a que evita la matriz de $n \\times n$, usando el producto elemento a elemento y la suma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c8795",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4cbe4",
   "metadata": {},
   "source": [
    "## Cholesky\n",
    "\n",
    "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
    "\n",
    "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
    "\n",
    "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$.\n",
    "\n",
    "### 3) Diferencias entre implementaciones de `QDA_Chol`\n",
    "\n",
    "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
    "\n",
    "Al aplicar la factorización de Cholesky a la matriz de convarianzas (matriz simétrica y definida positiva siempre que no hayan features perfectaemente colineales) se puede representar como $\\Sigma = $LL^T$, de tal manera que la expresión $(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)$ se puede reescribir como:\n",
    "\n",
    "$$\n",
    "(x-\\mu_j)^T (L_j L_j^T)^{-1} (x- \\mu_j)\n",
    "$$\n",
    "y operando un poco:\n",
    "$$\n",
    "(x-\\mu_j)^T (L_j^{-1})^T L_j^{-1} (x- \\mu_j)\n",
    "$$\n",
    "$$\n",
    "[L_j^{-1} (x- \\mu_j)]^T [L_j^{-1} (x- \\mu_j)]\n",
    "$$\n",
    "Si representamos $L_j^{-1} (x- \\mu_j)$ como $y$, la expresión puede verse como\n",
    "$$\n",
    "y^Ty = ||y||^2\n",
    "$$\n",
    "donde $y$ puede calcularse resolviendo un sistema de ecuaciones lineales triangular, que es relativamente sencillo con sustitución hacia adelante:\n",
    "$$\n",
    "L_j y = x- \\mu_j\n",
    "$$\n",
    "Por otro lado, respecto al logaritmo del determinante la matriz de covarianzas, se puede operar de la siguiente manera:\n",
    "$$\n",
    "\\log |\\Sigma_j| = \\log |L_j L_j^T| = \\log |L_j| + \\log |L_j^T| = 2 \\log |L_j|\n",
    "$$\n",
    "donde $|L_j|$, al ser $L_j$ una matriz triangular inferior, es el producto de sus elementos en la diagonal.\n",
    "\n",
    "Finalmente, dependiendo de la implementación elegida, la forma cuadrática de QDA se puede reescribir como:\n",
    "\n",
    "**Usando $L$** (QDA_Chol2):\n",
    "$$\\log{f_j(x)} = -\\log{ \\left( \\prod_{i=1}^p l_{ii} \\right)} - \\frac{1}{2} \\|y\\|^2$$\n",
    "donde $l_{ii}$ son los elementos diagonales de $L_j$\n",
    "\n",
    "**Usando $L^{-1}$** (QDA_Chol1, QDA_Chol3):\n",
    "$$\\log{f_j(x)} = \\log{ \\left( \\prod_{i=1}^p (L^{-1}_j)_{ii} \\right)} - \\frac{1}{2} \\|y\\|^2$$\n",
    "donde $(L^{-1}_j)_{ii}$ son los elementos diagonales de $L^{-1}_j$\n",
    "\n",
    "En resumen, aplicar Cholesky en QDA transforma un problema con  inversión de matrices en un problema de resolución de sistemas de ecuaciones lineales triangulares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ee83e",
   "metadata": {},
   "source": [
    "----\n",
    "7. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
    "\n",
    "`QDA` en su método `_fit_params` calcula e invierte la matriz de covarianzas para cada clase y las almacena en `self.inv_covs`.\n",
    "\n",
    "`QDA_Chol1`, en el mismo método, calcula la matriz de covarianzas y le aplica la factorización de Cholesky, obteniendo la matriz $L$. Luego, calcula la inversa de $L$ y la almacena en `self.L_invs`. Esto es para cada clase.\n",
    "\n",
    "A la hora de realizar una predición, en el método `_predict_log_conditional`, en `QDA` prácticamente utiliza la formulación provista inicialmente para el cálculo de la log-verosimilitud, con algunos cambios.\n",
    "$$\n",
    "\\log{f_j(x)} = \\frac{1}{2}\\log |\\Sigma_j^{-1}| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)\n",
    "$$\n",
    ", mientras que `QDA_Chol1` aprovecha, por un lado, la propiedad de que el determinante de $L^{-1}$ es el producto de los elementos de su diagonal y, por otro, la equivalencia $$(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) = \\|y\\|^2$$, tal que el cálculo de la log-verosimilitud resulta:\n",
    "$$\n",
    "\\log{f_j(x)} = \\log{ \\left( \\prod_{i=1}^p (L^{-1}_j)_{ii} \\right)} - \\frac{1}{2} \\|y\\|^2\n",
    "$$\n",
    "donde $(L^{-1}_j)_{ii}$ son los elementos diagonales de $L^{-1}_j$ (la clase almacena `L_invs`, no `Ls`), y así llegando ambos al mismo resultado porque todos las sustituciones corresponden a equivalencias matemáticas, sin utilizar aproximaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffcfbc4",
   "metadata": {},
   "source": [
    "8. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
    "\n",
    "`QDA_Chol1` y `QDA_Chol2` se diferencian en que la primera calcula la inversa de $L$ y luego, para predecir, utiliza el producto de esta matriz con $x$ insesgado para calcular $y$, mientras que la segunda se queda directamente con $L$ y, para predecir, resuelve el SEL triangular para obtener $y$.\n",
    "\n",
    "`QDA_Chol3` es similar a `QDA_Chol1`, pero utiliza otra función para calcular $L^{-1}$: `QDA_Chol1` utiliza `numpy.inv()` y `QDA_Chol3` utiliza `scipy.linalg.lapack.dtrtri()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65346b39",
   "metadata": {},
   "source": [
    "9. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84678165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c834e9a5d78414d9247532944dbc0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol1 (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ca15fdfbc144fcba81da0ec941236c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol1 (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39545aebc2b42c9b49d9e8972b45b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol2 (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e759fedb6736452ab6b881974c4dbb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol2 (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b4be7b92264edb99dcbaedf6d992a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol3 (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3a75a0d68d43e59c5fee5ecb4f242b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol3 (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Antes debe haberse ejecutado el bench de los otros modelos\n",
    "b.bench(QDA_Chol1)\n",
    "\n",
    "b.bench(QDA_Chol2)\n",
    "\n",
    "b.bench(QDA_Chol3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85463880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_median_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_median_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_median_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_median_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>8.16250</td>\n",
       "      <td>7.412496</td>\n",
       "      <td>1729.55460</td>\n",
       "      <td>1612.031217</td>\n",
       "      <td>0.886117</td>\n",
       "      <td>0.270096</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.099219</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>7.26470</td>\n",
       "      <td>0.616768</td>\n",
       "      <td>289.93255</td>\n",
       "      <td>3.242220</td>\n",
       "      <td>0.885303</td>\n",
       "      <td>0.268463</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.154099</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>1.123584</td>\n",
       "      <td>5.965369</td>\n",
       "      <td>1.006082</td>\n",
       "      <td>0.643866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>7.49610</td>\n",
       "      <td>1.225837</td>\n",
       "      <td>1095.82545</td>\n",
       "      <td>34.262841</td>\n",
       "      <td>0.884827</td>\n",
       "      <td>0.268951</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>258.234673</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>1.088900</td>\n",
       "      <td>1.578312</td>\n",
       "      <td>1.004255</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>7.14195</td>\n",
       "      <td>0.627668</td>\n",
       "      <td>11.95610</td>\n",
       "      <td>0.908321</td>\n",
       "      <td>0.884890</td>\n",
       "      <td>0.269440</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>15.743172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142895</td>\n",
       "      <td>144.658760</td>\n",
       "      <td>1.002435</td>\n",
       "      <td>0.006302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA_2</th>\n",
       "      <td>7.14635</td>\n",
       "      <td>1.065439</td>\n",
       "      <td>16.78935</td>\n",
       "      <td>1.380324</td>\n",
       "      <td>0.884770</td>\n",
       "      <td>0.268402</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>15.254768</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.142191</td>\n",
       "      <td>103.014983</td>\n",
       "      <td>1.006310</td>\n",
       "      <td>0.006504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol1</th>\n",
       "      <td>7.60500</td>\n",
       "      <td>1.187648</td>\n",
       "      <td>948.65465</td>\n",
       "      <td>37.157486</td>\n",
       "      <td>0.885433</td>\n",
       "      <td>0.269051</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.094734</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>1.073307</td>\n",
       "      <td>1.823166</td>\n",
       "      <td>1.003885</td>\n",
       "      <td>1.047339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol2</th>\n",
       "      <td>6.75825</td>\n",
       "      <td>0.586161</td>\n",
       "      <td>2340.74140</td>\n",
       "      <td>60.906744</td>\n",
       "      <td>0.885807</td>\n",
       "      <td>0.268723</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.095211</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>1.207783</td>\n",
       "      <td>0.738892</td>\n",
       "      <td>1.005110</td>\n",
       "      <td>1.042094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol3</th>\n",
       "      <td>7.25765</td>\n",
       "      <td>1.120846</td>\n",
       "      <td>947.43145</td>\n",
       "      <td>24.024118</td>\n",
       "      <td>0.884995</td>\n",
       "      <td>0.268723</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.094589</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>1.124675</td>\n",
       "      <td>1.825520</td>\n",
       "      <td>1.005110</td>\n",
       "      <td>1.048944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
       "model                                                                        \n",
       "QDA                     8.16250      7.412496      1729.55460  1612.031217   \n",
       "TensorizedQDA           7.26470      0.616768       289.93255     3.242220   \n",
       "FasterQDA               7.49610      1.225837      1095.82545    34.262841   \n",
       "EfficientQDA            7.14195      0.627668        11.95610     0.908321   \n",
       "EfficientQDA_2          7.14635      1.065439        16.78935     1.380324   \n",
       "QDA_Chol1               7.60500      1.187648       948.65465    37.157486   \n",
       "QDA_Chol2               6.75825      0.586161      2340.74140    60.906744   \n",
       "QDA_Chol3               7.25765      1.120846       947.43145    24.024118   \n",
       "\n",
       "                mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
       "model                                                                  \n",
       "QDA                  0.886117             0.270096          0.002008   \n",
       "TensorizedQDA        0.885303             0.268463          0.002143   \n",
       "FasterQDA            0.884827             0.268951          0.001919   \n",
       "EfficientQDA         0.884890             0.269440          0.002171   \n",
       "EfficientQDA_2       0.884770             0.268402          0.002159   \n",
       "QDA_Chol1            0.885433             0.269051          0.001967   \n",
       "QDA_Chol2            0.885807             0.268723          0.001848   \n",
       "QDA_Chol3            0.884995             0.268723          0.002190   \n",
       "\n",
       "                test_mem_median_mb  test_mem_std_mb  train_speedup  \\\n",
       "model                                                                \n",
       "QDA                       0.099219         0.001183       1.000000   \n",
       "TensorizedQDA             0.154099         0.000018       1.123584   \n",
       "FasterQDA               258.234673         0.000541       1.088900   \n",
       "EfficientQDA             15.743172         0.000000       1.142895   \n",
       "EfficientQDA_2           15.254768         0.000008       1.142191   \n",
       "QDA_Chol1                 0.094734         0.000349       1.073307   \n",
       "QDA_Chol2                 0.095211         0.000510       1.207783   \n",
       "QDA_Chol3                 0.094589         0.000429       1.124675   \n",
       "\n",
       "                test_speedup  train_mem_reduction  test_mem_reduction  \n",
       "model                                                                  \n",
       "QDA                 1.000000             1.000000            1.000000  \n",
       "TensorizedQDA       5.965369             1.006082            0.643866  \n",
       "FasterQDA           1.578312             1.004255            0.000384  \n",
       "EfficientQDA      144.658760             1.002435            0.006302  \n",
       "EfficientQDA_2    103.014983             1.006310            0.006504  \n",
       "QDA_Chol1           1.823166             1.003885            1.047339  \n",
       "QDA_Chol2           0.738892             1.005110            1.042094  \n",
       "QDA_Chol3           1.825520             1.005110            1.048944  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = b.summary(baseline=\"QDA\")\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216278d",
   "metadata": {},
   "source": [
    "`QDA_Chol1` parece ser la mejor de las implementaciones de Cholesky en cuanto a velocidad en train y test, superando al `QDA` base pero sin hacerlo respecto de `EfficientQDA`. `QDA_Chol2` incluso resulta ser más lento para predecir y `QDA_Chol3` es bastante parecida a la primera.\n",
    "\n",
    "Respecto a memoria, los 3 modelos con Cholesky tienen una leve mejora pero no parece significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ca153",
   "metadata": {},
   "source": [
    "### 4) Optimización\n",
    "\n",
    "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60ac268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedChol(QDA_Chol3):\n",
    "    def _fit_params(self, X, y):\n",
    "        super()._fit_params(X, y)\n",
    "\n",
    "        self.tensor_L_invs = np.stack(self.L_invs)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "\n",
    "    def _predict_log_conditionals(self, x):\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        y = self.tensor_L_invs @ unbiased_x\n",
    "\n",
    "        return np.log(self.tensor_L_invs.diagonal(axis1=1, axis2=2).prod(axis=1)) - 0.5 * (y**2).sum(axis=1).flatten()\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a14f70",
   "metadata": {},
   "source": [
    "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94f4ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientChol(TensorizedChol):\n",
    "    def _predict_log_conditionals_batch(self, X):\n",
    "        unbiased_X = X[None, :, :] - self.tensor_means\n",
    "\n",
    "        Y = self.tensor_L_invs @ unbiased_X\n",
    "\n",
    "        log_det_term = np.log(self.tensor_L_invs.diagonal(axis1=1, axis2=2).prod(axis=1))[:, None]\n",
    "        quad_term = (Y**2).sum(axis=1)\n",
    "        \n",
    "        return log_det_term - 0.5 * quad_term\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_conditionals = self._predict_log_conditionals_batch(X)\n",
    "        log_posteriori = self.log_a_priori[:, None] + log_conditionals\n",
    "\n",
    "        y_hat = np.argmax(log_posteriori, axis=0)\n",
    "\n",
    "        return y_hat.reshape(1, -1)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ffad1",
   "metadata": {},
   "source": [
    "14. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19955d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d300136c17e4fa6a6bc9af1f6238786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedChol (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fe4f8dd6114cd0a83b64099604feb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedChol (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6318a750d37e4c45b423da8416cd23f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EfficientChol (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25f443d7b0e43acbd76262f0fd4a8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EfficientChol (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b.bench(TensorizedChol)\n",
    "\n",
    "b.bench(EfficientChol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb2fadec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_median_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_median_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_median_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_median_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>8.16250</td>\n",
       "      <td>7.412496</td>\n",
       "      <td>1729.55460</td>\n",
       "      <td>1612.031217</td>\n",
       "      <td>0.886117</td>\n",
       "      <td>0.270096</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.099219</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>7.26470</td>\n",
       "      <td>0.616768</td>\n",
       "      <td>289.93255</td>\n",
       "      <td>3.242220</td>\n",
       "      <td>0.885303</td>\n",
       "      <td>0.268463</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.154099</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>1.123584</td>\n",
       "      <td>5.965369</td>\n",
       "      <td>1.006082</td>\n",
       "      <td>0.643866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>7.49610</td>\n",
       "      <td>1.225837</td>\n",
       "      <td>1095.82545</td>\n",
       "      <td>34.262841</td>\n",
       "      <td>0.884827</td>\n",
       "      <td>0.268951</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>258.234673</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>1.088900</td>\n",
       "      <td>1.578312</td>\n",
       "      <td>1.004255</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>7.14195</td>\n",
       "      <td>0.627668</td>\n",
       "      <td>11.95610</td>\n",
       "      <td>0.908321</td>\n",
       "      <td>0.884890</td>\n",
       "      <td>0.269440</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>15.743172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.142895</td>\n",
       "      <td>144.658760</td>\n",
       "      <td>1.002435</td>\n",
       "      <td>0.006302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA_2</th>\n",
       "      <td>7.14635</td>\n",
       "      <td>1.065439</td>\n",
       "      <td>16.78935</td>\n",
       "      <td>1.380324</td>\n",
       "      <td>0.884770</td>\n",
       "      <td>0.268402</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>15.254768</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.142191</td>\n",
       "      <td>103.014983</td>\n",
       "      <td>1.006310</td>\n",
       "      <td>0.006504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol1</th>\n",
       "      <td>7.60500</td>\n",
       "      <td>1.187648</td>\n",
       "      <td>948.65465</td>\n",
       "      <td>37.157486</td>\n",
       "      <td>0.885433</td>\n",
       "      <td>0.269051</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.094734</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>1.073307</td>\n",
       "      <td>1.823166</td>\n",
       "      <td>1.003885</td>\n",
       "      <td>1.047339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol2</th>\n",
       "      <td>6.75825</td>\n",
       "      <td>0.586161</td>\n",
       "      <td>2340.74140</td>\n",
       "      <td>60.906744</td>\n",
       "      <td>0.885807</td>\n",
       "      <td>0.268723</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.095211</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>1.207783</td>\n",
       "      <td>0.738892</td>\n",
       "      <td>1.005110</td>\n",
       "      <td>1.042094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol3</th>\n",
       "      <td>7.25765</td>\n",
       "      <td>1.120846</td>\n",
       "      <td>947.43145</td>\n",
       "      <td>24.024118</td>\n",
       "      <td>0.884995</td>\n",
       "      <td>0.268723</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.094589</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>1.124675</td>\n",
       "      <td>1.825520</td>\n",
       "      <td>1.005110</td>\n",
       "      <td>1.048944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedChol</th>\n",
       "      <td>7.70565</td>\n",
       "      <td>0.661728</td>\n",
       "      <td>74.05735</td>\n",
       "      <td>2.135236</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.268539</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.157776</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>1.059288</td>\n",
       "      <td>23.354260</td>\n",
       "      <td>1.005796</td>\n",
       "      <td>0.628859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientChol</th>\n",
       "      <td>7.96930</td>\n",
       "      <td>0.927994</td>\n",
       "      <td>16.73385</td>\n",
       "      <td>1.344136</td>\n",
       "      <td>0.885892</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>38.996689</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>1.024243</td>\n",
       "      <td>103.356645</td>\n",
       "      <td>1.002833</td>\n",
       "      <td>0.002544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
       "model                                                                        \n",
       "QDA                     8.16250      7.412496      1729.55460  1612.031217   \n",
       "TensorizedQDA           7.26470      0.616768       289.93255     3.242220   \n",
       "FasterQDA               7.49610      1.225837      1095.82545    34.262841   \n",
       "EfficientQDA            7.14195      0.627668        11.95610     0.908321   \n",
       "EfficientQDA_2          7.14635      1.065439        16.78935     1.380324   \n",
       "QDA_Chol1               7.60500      1.187648       948.65465    37.157486   \n",
       "QDA_Chol2               6.75825      0.586161      2340.74140    60.906744   \n",
       "QDA_Chol3               7.25765      1.120846       947.43145    24.024118   \n",
       "TensorizedChol          7.70565      0.661728        74.05735     2.135236   \n",
       "EfficientChol           7.96930      0.927994        16.73385     1.344136   \n",
       "\n",
       "                mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
       "model                                                                  \n",
       "QDA                  0.886117             0.270096          0.002008   \n",
       "TensorizedQDA        0.885303             0.268463          0.002143   \n",
       "FasterQDA            0.884827             0.268951          0.001919   \n",
       "EfficientQDA         0.884890             0.269440          0.002171   \n",
       "EfficientQDA_2       0.884770             0.268402          0.002159   \n",
       "QDA_Chol1            0.885433             0.269051          0.001967   \n",
       "QDA_Chol2            0.885807             0.268723          0.001848   \n",
       "QDA_Chol3            0.884995             0.268723          0.002190   \n",
       "TensorizedChol       0.886000             0.268539          0.001642   \n",
       "EfficientChol        0.885892             0.269333          0.001765   \n",
       "\n",
       "                test_mem_median_mb  test_mem_std_mb  train_speedup  \\\n",
       "model                                                                \n",
       "QDA                       0.099219         0.001183       1.000000   \n",
       "TensorizedQDA             0.154099         0.000018       1.123584   \n",
       "FasterQDA               258.234673         0.000541       1.088900   \n",
       "EfficientQDA             15.743172         0.000000       1.142895   \n",
       "EfficientQDA_2           15.254768         0.000008       1.142191   \n",
       "QDA_Chol1                 0.094734         0.000349       1.073307   \n",
       "QDA_Chol2                 0.095211         0.000510       1.207783   \n",
       "QDA_Chol3                 0.094589         0.000429       1.124675   \n",
       "TensorizedChol            0.157776         0.000201       1.059288   \n",
       "EfficientChol            38.996689         0.000134       1.024243   \n",
       "\n",
       "                test_speedup  train_mem_reduction  test_mem_reduction  \n",
       "model                                                                  \n",
       "QDA                 1.000000             1.000000            1.000000  \n",
       "TensorizedQDA       5.965369             1.006082            0.643866  \n",
       "FasterQDA           1.578312             1.004255            0.000384  \n",
       "EfficientQDA      144.658760             1.002435            0.006302  \n",
       "EfficientQDA_2    103.014983             1.006310            0.006504  \n",
       "QDA_Chol1           1.823166             1.003885            1.047339  \n",
       "QDA_Chol2           0.738892             1.005110            1.042094  \n",
       "QDA_Chol3           1.825520             1.005110            1.048944  \n",
       "TensorizedChol     23.354260             1.005796            0.628859  \n",
       "EfficientChol     103.356645             1.002833            0.002544  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = b.summary(baseline=\"QDA\")\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dee61a",
   "metadata": {},
   "source": [
    "TODO: Conclusión final (tentativo)\n",
    "\n",
    "## Conclusiones finales\n",
    "\n",
    "Se puede observar que `EfficientChol` combina las mejores optimizaciones:\n",
    "- **Entrenamiento más rápido** que QDA base gracias a Cholesky\n",
    "- **Predicción más rápida** al paralelizar sobre k clases Y n observaciones\n",
    "- **Menor consumo de memoria** al evitar la matriz N×N\n",
    "\n",
    "Este modelo representa la mejor implementación, eliminando todos los loops \n",
    "explícitos (tanto sobre clases como sobre observaciones) y aprovechando las \n",
    "ventajas numéricas de la descomposición de Cholesky.\n",
    "\n",
    "Comparado con QDA base:\n",
    "- Entrenamiento: [X]% más rápido\n",
    "- Predicción: [Y]% más rápido  \n",
    "- Memoria: [Z]% menos consumo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
