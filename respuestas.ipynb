{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ce50a8",
   "metadata": {},
   "source": [
    "## RESPUESTAS \n",
    "#### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
    "\n",
    "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
    "\n",
    "Sobre las $k$ clases.\n",
    "\n",
    "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
    "\n",
    "`tensor_inv_covs` es un array de NumPy de $\\mathbb{R}^{k \\times p \\times n}$\n",
    "\n",
    "`tensor_means` es un array de NumPy de $\\mathbb{R}^{k \\times p \\times 1}$\n",
    "\n",
    "QDA en el método `_predict_one` itera sobre las $k$ clases calculando el logaritmo de la probabilidad a posteriori para cada una, usando el método `_predict_log_conditional` (este método devuelve un escalar), obteniendo una lista (de tamaño $k$) con la probabilidad de que la observación pertenezca a cada una de las clases. Luego, devuelve el argumento de la mayor probabilidad en dicha lista.\n",
    "\n",
    "inv_cov $\\in \\mathbb{R}^{p \\times p}$\n",
    "\n",
    "means, ubiased_x $\\in \\mathbb{R}^{p \\times 1}$\n",
    "\n",
    "unbiased_x.T @ inv_cov @ unbiased_x $\\rarr$ $(1 \\times p) @ (p \\times p) @ (p \\times 1) $ $\\in \\mathbb{R}^{1 \\times 1} \\rarr $ escalar\n",
    "\n",
    "Entonces, `_predict_log_conditional` devuelve un escalar.\n",
    "\n",
    "TensorizedQDA directamente hace el cálculo de los logaritmos de las probabilidades a posteriori de forma matricial en el método `_predict_log_conditionals`, el cual directamente devuelve un vector columna $\\mathbb{R}^{p \\times 1}$, similar a la lista obtenida con el bucle for dentro del método `_predict_one` de la clase `QDA`. Luego, el método `_predict_one` para esta clase solo se encarga de encotrar el argumento que maximiza la probailidad (logarítmica).\n",
    "\n",
    "tensor_inv_cov # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7653d5",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71311f41",
   "metadata": {},
   "source": [
    "### 2) Optimización\n",
    "\n",
    "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
    "\n",
    "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246eaf3",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "Poder eliminar el ciclo for de predict. Es decir, predecir las n observaciones y k clases en un mismo paso. \n",
    "\n",
    "En BaseBayesianClassifier\n",
    "\n",
    "predict(self, X) → tiene ciclo for recorre las n observaciones, y llama a predict_one, donde TensorizedQDA ya paraleliza las k clases. \n",
    "\n",
    "-------------\n",
    "def predict(self, X): → llama a predict_one por cada observación\n",
    "\n",
    "def _predict_one(self, x): → llama a _predict_log_conditional para hacer el argmax de la suma\n",
    "\n",
    "$$\n",
    "\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j\n",
    "$$\n",
    "\n",
    "def _predict_log_conditional(self, x, class_idx)\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "----------------\n",
    "\n",
    "Buscamos calcular la forma cuadrática para muchas observaciones a la vez:\n",
    "\n",
    "$$(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$$\n",
    "\n",
    "Donde usamos : _predict_log_conditional(x, class_idx)\n",
    "\n",
    "unbiased_x = x - mean_j\n",
    "\n",
    "unbiased_x.T @ inv_cov_j @ unbiased_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0bc62",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy        as np\n",
    "import pandas       as pd\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg           import cholesky, solve_triangular\n",
    "from scipy.linalg.lapack    import dtrtri\n",
    "\n",
    "from base.qda               import QDA, TensorizedQDA\n",
    "from base.cholesky          import QDA_Chol1, QDA_Chol2, QDA_Chol3\n",
    "from utils.bench            import Benchmark\n",
    "from utils.datasets         import (get_letters_dataset,label_encode)                                                     \n",
    "from numpy.random           import RandomState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14291914",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50995c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benching params:\n",
      "Total runs: 150\n",
      "Warmup runs: 20\n",
      "Peak Memory usage runs: 30\n",
      "Running time runs: 100\n",
      "Train size rows (approx): 16000\n",
      "Test size rows (approx): 4000\n",
      "Test size fraction: 0.2\n"
     ]
    }
   ],
   "source": [
    "# dataset de letters\n",
    "X_letter, y_letter = get_letters_dataset()\n",
    "\n",
    "# encoding de labels\n",
    "y_letter_encoded = label_encode(y_letter.reshape(-1,1)) # hago reshape para que quede como matriz columna\n",
    "\n",
    "# instanciacion del benchmark\n",
    "b = Benchmark(\n",
    "    X_letter, y_letter_encoded,\n",
    "    same_splits=False,\n",
    "    n_runs=100,\n",
    "    warmup=20,\n",
    "    mem_runs=30,\n",
    "    test_sz=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810f7778",
   "metadata": {},
   "source": [
    "### Prueba QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735a04a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  5, 18,  7,  7]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QDA()\n",
    "\n",
    "qda.fit(X_letter.T, y_letter_encoded)\n",
    "\n",
    "qda.predict(X_letter.T[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24cec0",
   "metadata": {},
   "source": [
    "### Prueba TensorizedQDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3b54275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  5, 18,  7,  7]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqda = TensorizedQDA()\n",
    "\n",
    "tqda.fit(X_letter.T, y_letter_encoded)\n",
    "\n",
    "tqda.predict(X_letter.T[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634477c4",
   "metadata": {},
   "source": [
    "### FasterQDA\n",
    "\n",
    "Definir una clase FasterQDA que herede de TensorizedQDA y redefina predict para predecir todas las observaciones juntas, sin el for sobre filas de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b21da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    \"\"\"\n",
    "    Versión vectorizada (sin bucles en predict), \n",
    "    con matriz intermedia ineficiente (matriz N x N).\n",
    "    \"\"\"\n",
    "\n",
    "    def _predict_log_conditionals_batch(self, X):\n",
    "        # Dimensiones iniciales\n",
    "        k, p, _ = self.tensor_means.shape # self.tensor_means: (k, p, 1)    → k clases, p features\n",
    "        n = X.shape[1]                    # X: (p, n)                       → p features, n observaciones\n",
    "\n",
    "        # Cálculo resta: D = X - medias\n",
    "        D = X[None, :, :] - self.tensor_means   # X: (1, p, n) - Medias: (k, p, 1) → D: (k, p, n)\n",
    "\n",
    "        # Cálculo de la distancia cuadrática \n",
    "        quad_terms = np.empty((k, n)) # Almacena distancias por cada k clase y observación n\n",
    "\n",
    "        for j in range(k):\n",
    "            D_j = D[j]                          # Shape: (p, n)\n",
    "            inv_cov_j = self.tensor_inv_cov[j]  # Shape: (p, p)\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # Generación de matriz n x n\n",
    "            # ---------------------------------------------------------\n",
    "            # Al multiplicar D_j.T (n, p) @ inv (p, p) @ D_j (p, n), el resultado final es una matriz de (n, n)\n",
    "            # solo nos importa cada dato consigo mismo (la diagonal)\n",
    "            \n",
    "            Q_j = D_j.T @ inv_cov_j @ D_j      # matriz (n, n)\n",
    "\n",
    "            # Nos quedamos con la diagonal \n",
    "            quad_terms[j, :] = np.diag(Q_j)    \n",
    "            # ---------------------------------------------------------\n",
    "\n",
    "        \n",
    "        # log_det es (k,). Lo convertimos a (k, 1) para operar con quad_terms\n",
    "        log_det = np.log(LA.det(self.tensor_inv_cov))[:, None]\n",
    "        \n",
    "        # Fórmula final: 0.5 * log_det - 0.5 * distancia\n",
    "        return 0.5 * log_det - 0.5 * quad_terms\n",
    "\n",
    "    def predict(self, X):\n",
    "        # traigo el log-condicionales \n",
    "        log_cond = self._predict_log_conditionals_batch(X)\n",
    "\n",
    "        # Sumamos el log_a_priori \n",
    "        log_post = self.log_a_priori[:, None] + log_cond\n",
    "\n",
    "        # Elegimos la clase ganadora → por columna\n",
    "        y_hat = np.argmax(log_post, axis=0)\n",
    "\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553097f6",
   "metadata": {},
   "source": [
    "## Comparar modelos con benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa2cf8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fedc0f4e7a46a9b4596357cffc6dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd84cd920904d488b15ab66828a4cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79568b1a27d14212aa9e9efc758b8169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedQDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c4fcabe4004012891f97a06bf8a229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedQDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e54af0dee3041e6ac11947e14870b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FasterQDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e655fa86f64c17a53ec3150f2bbf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FasterQDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QDA \n",
    "b.bench(QDA)\n",
    "\n",
    "# TensorizedQDA\n",
    "b.bench(TensorizedQDA)\n",
    "\n",
    "# FasterQDA\n",
    "b.bench(FasterQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119f3fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_median_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_median_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_median_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_median_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>8.56435</td>\n",
       "      <td>2.054508</td>\n",
       "      <td>1812.28355</td>\n",
       "      <td>392.645214</td>\n",
       "      <td>0.886117</td>\n",
       "      <td>0.270096</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.098967</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>8.68600</td>\n",
       "      <td>1.364361</td>\n",
       "      <td>314.37560</td>\n",
       "      <td>33.107114</td>\n",
       "      <td>0.885303</td>\n",
       "      <td>0.268463</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.154099</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.985995</td>\n",
       "      <td>5.764708</td>\n",
       "      <td>1.006082</td>\n",
       "      <td>0.642229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>10.12145</td>\n",
       "      <td>0.943249</td>\n",
       "      <td>1922.09195</td>\n",
       "      <td>26.821284</td>\n",
       "      <td>0.884827</td>\n",
       "      <td>0.268951</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>258.234673</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.846158</td>\n",
       "      <td>0.942870</td>\n",
       "      <td>1.004255</td>\n",
       "      <td>0.000383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
       "model                                                                       \n",
       "QDA                    8.56435      2.054508      1812.28355   392.645214   \n",
       "TensorizedQDA          8.68600      1.364361       314.37560    33.107114   \n",
       "FasterQDA             10.12145      0.943249      1922.09195    26.821284   \n",
       "\n",
       "               mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
       "model                                                                 \n",
       "QDA                 0.886117             0.270096          0.002008   \n",
       "TensorizedQDA       0.885303             0.268463          0.002143   \n",
       "FasterQDA           0.884827             0.268951          0.001919   \n",
       "\n",
       "               test_mem_median_mb  test_mem_std_mb  train_speedup  \\\n",
       "model                                                               \n",
       "QDA                      0.098967         0.000588       1.000000   \n",
       "TensorizedQDA            0.154099         0.000152       0.985995   \n",
       "FasterQDA              258.234673         0.000796       0.846158   \n",
       "\n",
       "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
       "model                                                                 \n",
       "QDA                1.000000             1.000000            1.000000  \n",
       "TensorizedQDA      5.764708             1.006082            0.642229  \n",
       "FasterQDA          0.942870             1.004255            0.000383  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = b.summary(baseline=\"QDA\")\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de007667",
   "metadata": {},
   "source": [
    "Vemos que TensorizedQDA entrena en el mismo tiempo que el QDA base y clasifica con la misma accuracy, pero es mucho más rápido en predicción.\n",
    "FasterQDA no lo mejora porque empieza a comparar todas las observaciones contra todas y arma una matriz gigante N×N que consume 258 MB de RAM y ralentiza todo el proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c8795",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4cbe4",
   "metadata": {},
   "source": [
    "## Cholesky\n",
    "\n",
    "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
    "\n",
    "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
    "\n",
    "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$.\n",
    "\n",
    "### 3) Diferencias entre implementaciones de `QDA_Chol`\n",
    "\n",
    "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
    "\n",
    "Al aplicar la factorización de Cholesky a la matriz de convarianzas (matriz simétrica y definida positiva siempre que no hayan features perfectaemente colineales) se puede representar como $\\Sigma = $LL^T$, de tal manera que la expresión $(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)$ se puede reescribir como:\n",
    "\n",
    "$$\n",
    "(x-\\mu_j)^T (L_j L_j^T)^{-1} (x- \\mu_j)\n",
    "$$\n",
    "y operando un poco:\n",
    "$$\n",
    "(x-\\mu_j)^T (L_j^{-1})^T L_j^{-1} (x- \\mu_j)\n",
    "$$\n",
    "$$\n",
    "[L_j^{-1} (x- \\mu_j)]^T [L_j^{-1} (x- \\mu_j)]\n",
    "$$\n",
    "Si representamos $L_j^{-1} (x- \\mu_j)$ como $y$, la expresión puede verse como\n",
    "$$y^Ty = ||y||^2\n",
    "$$\n",
    "\n",
    "7. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
    "8. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
    "9. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TP_AMIA",
   "language": "python",
   "name": "tp_amia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
