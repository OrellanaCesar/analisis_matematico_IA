{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ce50a8",
   "metadata": {},
   "source": [
    "## RESPUESTAS \n",
    "#### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
    "\n",
    "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
    "\n",
    "Sobre las $k$ clases.\n",
    "\n",
    "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
    "\n",
    "`tensor_inv_covs` es un array de NumPy de $\\mathbb{R}^{k \\times p \\times n}$\n",
    "\n",
    "`tensor_means` es un array de NumPy de $\\mathbb{R}^{k \\times p \\times 1}$\n",
    "\n",
    "QDA en el método `_predict_one` itera sobre las $k$ clases calculando el logaritmo de la probabilidad a posteriori para cada una, usando el método `_predict_log_conditional` (este método devuelve un escalar), obteniendo una lista (de tamaño $k$) con la probabilidad de que la observación pertenezca a cada una de las clases. Luego, devuelve el argumento de la mayor probabilidad en dicha lista.\n",
    "\n",
    "QDA\n",
    "inv_cov # (p,p)\n",
    "unbiased_x # (p,1)\n",
    "return 0.5*np.log(LA.det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x\n",
    "(1,p) @ (p,p) @ (p,1) → (1,1) → escalar.\n",
    "\n",
    "TensorizedQDA directamente hace el cálculo de los logaritmos de las probabilidades a posteriori de forma matricial en el método `_predict_log_conditionals`, el cual directamente devuelve un vector columna $\\mathbb{R}^{p \\times 1}$, similar a la lista obtenida con el bucle for dentro del método `_predict_one` de la clase `QDA`. Luego, el método `_predict_one` para esta clase solo se encarga de encotrar el argumento que maximiza la probailidad (logarítmica).\n",
    "\n",
    "tensor_inv_cov # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71311f41",
   "metadata": {},
   "source": [
    "### 2) Optimización\n",
    "\n",
    "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
    "\n",
    "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246eaf3",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "Poder eliminar el ciclo for de predict. Es decir, predecir las n observaciones y k clases en un mismo paso. \n",
    "\n",
    "En BaseBayesianClassifier\n",
    "\n",
    "predict(self, X) → tiene ciclo for recorre las n observaciones, y llama a predict_one, donde TensorizedQDA ya paraleliza las k clases. \n",
    "\n",
    "-------------\n",
    "def predict(self, X): → llama a predict_one por cada observación\n",
    "\n",
    "def _predict_one(self, x): → llama a _predict_log_conditional para hacer el argmax de la suma\n",
    "\n",
    "$$\n",
    "\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j\n",
    "$$\n",
    "\n",
    "def _predict_log_conditional(self, x, class_idx)\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "----------------\n",
    "\n",
    "Buscamos calcular la forma cuadrática para muchas observaciones a la vez:\n",
    "\n",
    "$$(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$$\n",
    "\n",
    "Donde usamos : _predict_log_conditional(x, class_idx)\n",
    "\n",
    "unbiased_x = x - mean_j\n",
    "\n",
    "unbiased_x.T @ inv_cov_j @ unbiased_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0bc62",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa2e0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy        as np\n",
    "import pandas       as pd\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg           import cholesky, solve_triangular\n",
    "from scipy.linalg.lapack    import dtrtri\n",
    "\n",
    "from base.qda               import QDA, TensorizedQDA\n",
    "from base.cholesky          import QDA_Chol1, QDA_Chol2, QDA_Chol3\n",
    "from utils.bench            import Benchmark\n",
    "from utils.datasets         import (get_letters_dataset,label_encode)                                                     \n",
    "from numpy.random           import RandomState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14291914",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50995c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benching params:\n",
      "Total runs: 150\n",
      "Warmup runs: 20\n",
      "Peak Memory usage runs: 30\n",
      "Running time runs: 100\n",
      "Train size rows (approx): 16000\n",
      "Test size rows (approx): 4000\n",
      "Test size fraction: 0.2\n"
     ]
    }
   ],
   "source": [
    "# dataset de letters\n",
    "X_letter, y_letter = get_letters_dataset()\n",
    "\n",
    "# encoding de labels\n",
    "y_letter_encoded = label_encode(y_letter.reshape(-1,1)) # hago reshape para que quede como matriz columna\n",
    "\n",
    "# instanciacion del benchmark\n",
    "b = Benchmark(\n",
    "    X_letter, y_letter_encoded,\n",
    "    same_splits=False,\n",
    "    n_runs=100,\n",
    "    warmup=20,\n",
    "    mem_runs=30,\n",
    "    test_sz=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810f7778",
   "metadata": {},
   "source": [
    "### Prueba QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "735a04a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  5, 18,  7,  7]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QDA()\n",
    "\n",
    "qda.fit(X_letter.T, y_letter_encoded)\n",
    "\n",
    "qda.predict(X_letter.T[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24cec0",
   "metadata": {},
   "source": [
    "### Prueba TensorizedQDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3b54275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  5, 18,  7,  7]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqda = TensorizedQDA()\n",
    "\n",
    "tqda.fit(X_letter.T, y_letter_encoded)\n",
    "\n",
    "tqda.predict(X_letter.T[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634477c4",
   "metadata": {},
   "source": [
    "### FasterQDA\n",
    "\n",
    "Definir una clase FasterQDA que herede de TensorizedQDA y redefina predict para predecir todas las observaciones juntas, sin el for sobre filas de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b21da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    \"\"\"\n",
    "    Versión vectorizada (sin bucles en predict), \n",
    "    con matriz intermedia ineficiente (matriz N x N).\n",
    "    \"\"\"\n",
    "\n",
    "    def _predict_log_conditionals_batch(self, X):\n",
    "        # Dimensiones iniciales\n",
    "        k, p, _ = self.tensor_means.shape # self.tensor_means: (k, p, 1)    → k clases, p features\n",
    "        n = X.shape[1]                    # X: (p, n)                       → p features, n observaciones\n",
    "\n",
    "        # Cálculo resta: D = X - medias\n",
    "        D = X[None, :, :] - self.tensor_means   # X: (1, p, n) - Medias: (k, p, 1) → D: (k, p, n)\n",
    "\n",
    "        # Cálculo de la distancia cuadrática \n",
    "        quad_terms = np.empty((k, n)) # Almacena distancias por cada k clase y observación n\n",
    "\n",
    "        for j in range(k):\n",
    "            D_j = D[j]                          # Shape: (p, n)\n",
    "            inv_cov_j = self.tensor_inv_cov[j]  # Shape: (p, p)\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # Generación de matriz n x n\n",
    "            # ---------------------------------------------------------\n",
    "            # Al multiplicar D_j.T (n, p) @ inv (p, p) @ D_j (p, n), el resultado final es una matriz de (n, n)\n",
    "            # solo nos importa cada dato consigo mismo (la diagonal)\n",
    "            \n",
    "            Q_j = D_j.T @ inv_cov_j @ D_j      # matriz (n, n)\n",
    "\n",
    "            # Nos quedamos con la diagonal \n",
    "            quad_terms[j, :] = np.diag(Q_j)    \n",
    "            # ---------------------------------------------------------\n",
    "\n",
    "        \n",
    "        # log_det es (k,). Lo convertimos a (k, 1) para operar con quad_terms\n",
    "        log_det = np.log(LA.det(self.tensor_inv_cov))[:, None]\n",
    "        \n",
    "        # Fórmula final: 0.5 * log_det - 0.5 * distancia\n",
    "        return 0.5 * log_det - 0.5 * quad_terms\n",
    "\n",
    "    def predict(self, X):\n",
    "        # traigo el log-condicionales \n",
    "        log_cond = self._predict_log_conditionals_batch(X)\n",
    "\n",
    "        # Sumamos el log_a_priori \n",
    "        log_post = self.log_a_priori[:, None] + log_cond\n",
    "\n",
    "        # Elegimos la clase ganadora → por columna\n",
    "        y_hat = np.argmax(log_post, axis=0)\n",
    "\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce3b42",
   "metadata": {},
   "source": [
    "2.5) Demostrar que\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$ es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
    "$$\n",
    "np.sum(A^T \\odot B, axis=0).T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957ed46",
   "metadata": {},
   "source": [
    "# Demostración paso a paso\n",
    "\n",
    "### Producto matricial\n",
    "\n",
    "El elemento $(i,j)$ de $A \\cdot B$ se calcula como:\n",
    "$$\n",
    "(A \\cdot B)_{ij} = \\sum_{k=1}^{p} A_{ik} \\cdot B_{kj}\n",
    "$$\n",
    "\n",
    "Es decir, el **producto interno** entre la fila $i$ de $A$ y la columna $j$ de $B$.\n",
    "\n",
    "Para la diagonal, solo nos interesan los elementos donde $i = j$:\n",
    "$$\n",
    "(A \\cdot B)_{ii} = \\sum_{k=1}^{p} A_{ik} \\cdot B_{ki}\n",
    "$$\n",
    "\n",
    "Se observa que $B_{ki}$ es el elemento en la posición $(k, i)$ de $B$, que es exactamente el elemento $(i, k)$ de $B^T$.\n",
    "\n",
    "Por lo tanto:\n",
    "$$\n",
    "(A \\cdot B)_{ii} = \\sum_{k=1}^{p} A_{ik} \\cdot (B^T)_{ik}\n",
    "$$\n",
    "\n",
    "\n",
    "Si hacemos el producto elemento a elemento (Hadamard) $A \\odot B^T$ :\n",
    "$$\n",
    "(A \\odot B^T)_{ik} = A_{ik} \\cdot (B^T)_{ik}\n",
    "$$\n",
    "\n",
    "Y luego sumamos a lo largo de las columnas (axis=1):\n",
    "$$\n",
    "\\sum_{k=1}^{p} (A \\odot B^T)_{ik} = \\sum_{k=1}^{p}  A_{ik} \\cdot (B^T)_{ik}    = (A \\cdot B)_{ii}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bce70",
   "metadata": {},
   "source": [
    "### EJEMPLO matriz 2x2\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{pmatrix}\n",
    "\\quad\n",
    "B = \\begin{pmatrix}\n",
    "e & f \\\\\n",
    "g & h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "###  $A \\cdot B$\n",
    "\n",
    "$$\n",
    "A \\cdot B = \\begin{pmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "e & f \\\\\n",
    "g & h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A \\cdot B = \\begin{pmatrix}\n",
    "ae + bg & af + bh \\\\\n",
    "ce + dg & cf + dh\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "###  La diagonal\n",
    "\n",
    "$$\n",
    "\\text{diag}(A \\cdot B) = \\begin{pmatrix}\n",
    "ae + bg \\\\\n",
    "cf + dh\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  $B^T$\n",
    "\n",
    "$$\n",
    "B^T = \\begin{pmatrix}\n",
    "e & g \\\\\n",
    "f & h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### $A \\odot B^T$\n",
    "\n",
    "$$\n",
    "A \\odot B^T = \\begin{pmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{pmatrix}\n",
    "\\odot\n",
    "\\begin{pmatrix}\n",
    "e & g \\\\\n",
    "f & h\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "a \\cdot e & b \\cdot g \\\\\n",
    "c \\cdot f & d \\cdot h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Suma cada fila (axis=1)\n",
    "\n",
    "$$\n",
    "\\text{np.sum}(A \\odot B^T, \\text{axis}=1) = \\begin{pmatrix}\n",
    "ae + bg \\\\\n",
    "cf + dh\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\text{diag}(A \\cdot B) = \\begin{pmatrix}\n",
    "ae + bg \\\\\n",
    "cf + dh\n",
    "\\end{pmatrix}\n",
    "= \\text{np.sum}(A \\odot B^T, \\text{axis}=1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95ae8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientQDA(TensorizedQDA):\n",
    "    \"\"\"\n",
    "    Versión que evita la matriz nxn\n",
    "    usando el producto de Hadamard para calcular solo la diagonal.\n",
    "    \"\"\"\n",
    "\n",
    "    def _predict_log_conditionals_batch(self, X):\n",
    "        # Dimensiones iniciales\n",
    "        k, p, _ = self.tensor_means.shape  # self.tensor_means: (k, p, 1) → k clases, p features\n",
    "        n = X.shape[1]                     # X: (p, n)                    → p features, n observaciones\n",
    "\n",
    "        # Cálculo resta: D = X - medias\n",
    "        D = X[None, :, :] - self.tensor_means   # X: (1, p, n) - Medias: (k, p, 1) → D: (k, p, n)\n",
    "\n",
    "        # Cálculo de la distancia cuadrática \n",
    "        quad_terms = np.empty((k, n))  # Almacena distancias por cada k clase y observación n\n",
    "\n",
    "        for j in range(k):\n",
    "            D_j = D[j]                          # Shape: (p, n)\n",
    "            inv_cov_j = self.tensor_inv_cov[j]  # Shape: (p, p)\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # MÉTODO EFICIENTE: usando producto de Hadamard\n",
    "            # ---------------------------------------------------------\n",
    "            # En vez de hacerr Q_j = D_j.T @ inv_cov_j @ D_j  → (n, n)\n",
    "            # Calculamos lo siguiente diag(A @ B) = sum(A ⊙ B.T, axis=1)\n",
    "            \n",
    "            # 1) A = D_j.T @ inv_cov_j  → (n, p)\n",
    "            A = D_j.T @ inv_cov_j\n",
    "            \n",
    "            # 2) B = D_j  → (p, n), entonces B.T = D_j.T → (n, p)\n",
    "            # 3) diag(A @ D_j) = sum(A ⊙ D_j.T, axis=1)\n",
    "            quad_terms[j, :] = np.sum(A * D_j.T, axis=1)\n",
    "            # ---------------------------------------------------------\n",
    "\n",
    "        # log_det es (k,). Lo convertimos a (k, 1) para operar con quad_terms\n",
    "        log_det = np.log(LA.det(self.tensor_inv_cov))[:, None]\n",
    "        \n",
    "        # Fórmula final: 0.5 * log_det - 0.5 * distancia\n",
    "        return 0.5 * log_det - 0.5 * quad_terms\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Traigo los log-condicionales \n",
    "        log_cond = self._predict_log_conditionals_batch(X)\n",
    "\n",
    "        # Sumamos el log_a_priori \n",
    "        log_post = self.log_a_priori[:, None] + log_cond\n",
    "\n",
    "        # Elegimos la clase ganadora → por columna\n",
    "        y_hat = np.argmax(log_post, axis=0)\n",
    "\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553097f6",
   "metadata": {},
   "source": [
    "## Comparar modelos con benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfa2cf8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36fab6aecad433b94556e2ecfd59957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be17c07274f649dbbaea555c76f1f8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047ec3dba510481989b998da2848b7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedQDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94a7791038945ed8bf50da8f51ca760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedQDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d83d482a0674f13b70894ecd7fa541a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FasterQDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88a6fb5a1b24d4b85f5b333e6740e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FasterQDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0e40b9f3bc4844a3634f0f7c2388e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EfficientQDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cccdc00315488fb56298d3f9d0e385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EfficientQDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QDA \n",
    "b.bench(QDA)\n",
    "\n",
    "# TensorizedQDA\n",
    "b.bench(TensorizedQDA)\n",
    "\n",
    "# FasterQDA\n",
    "b.bench(FasterQDA)\n",
    "\n",
    "# EfficientQDA\n",
    "b.bench(EfficientQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "119f3fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_median_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_median_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_median_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_median_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>9.6848</td>\n",
       "      <td>1.976466</td>\n",
       "      <td>1734.41890</td>\n",
       "      <td>159.250814</td>\n",
       "      <td>0.886117</td>\n",
       "      <td>0.270096</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.098543</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>9.1615</td>\n",
       "      <td>1.756816</td>\n",
       "      <td>300.54200</td>\n",
       "      <td>50.063407</td>\n",
       "      <td>0.885303</td>\n",
       "      <td>0.268463</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.154099</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.057119</td>\n",
       "      <td>5.770970</td>\n",
       "      <td>1.006082</td>\n",
       "      <td>0.639479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>9.8552</td>\n",
       "      <td>3.432659</td>\n",
       "      <td>1921.78170</td>\n",
       "      <td>111.842834</td>\n",
       "      <td>0.884827</td>\n",
       "      <td>0.268951</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>258.234673</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.982710</td>\n",
       "      <td>0.902506</td>\n",
       "      <td>1.004255</td>\n",
       "      <td>0.000382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>7.3710</td>\n",
       "      <td>1.207504</td>\n",
       "      <td>11.51395</td>\n",
       "      <td>1.288257</td>\n",
       "      <td>0.884890</td>\n",
       "      <td>0.269440</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>15.743172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.313906</td>\n",
       "      <td>150.636306</td>\n",
       "      <td>1.002435</td>\n",
       "      <td>0.006259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
       "model                                                                       \n",
       "QDA                     9.6848      1.976466      1734.41890   159.250814   \n",
       "TensorizedQDA           9.1615      1.756816       300.54200    50.063407   \n",
       "FasterQDA               9.8552      3.432659      1921.78170   111.842834   \n",
       "EfficientQDA            7.3710      1.207504        11.51395     1.288257   \n",
       "\n",
       "               mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
       "model                                                                 \n",
       "QDA                 0.886117             0.270096          0.002008   \n",
       "TensorizedQDA       0.885303             0.268463          0.002143   \n",
       "FasterQDA           0.884827             0.268951          0.001919   \n",
       "EfficientQDA        0.884890             0.269440          0.002171   \n",
       "\n",
       "               test_mem_median_mb  test_mem_std_mb  train_speedup  \\\n",
       "model                                                               \n",
       "QDA                      0.098543         0.001085       1.000000   \n",
       "TensorizedQDA            0.154099         0.000115       1.057119   \n",
       "FasterQDA              258.234673         0.000689       0.982710   \n",
       "EfficientQDA            15.743172         0.000000       1.313906   \n",
       "\n",
       "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
       "model                                                                 \n",
       "QDA                1.000000             1.000000            1.000000  \n",
       "TensorizedQDA      5.770970             1.006082            0.639479  \n",
       "FasterQDA          0.902506             1.004255            0.000382  \n",
       "EfficientQDA     150.636306             1.002435            0.006259  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = b.summary(baseline=\"QDA\")\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de007667",
   "metadata": {},
   "source": [
    "Vemos que TensorizedQDA entrena en el mismo tiempo que el QDA base y clasifica con la misma accuracy, pero es mucho más rápido en predicción.\n",
    "FasterQDA no lo mejora porque empieza a comparar todas las observaciones contra todas y arma una matriz gigante N×N que consume 258 MB de RAM y ralentiza todo el proceso.\n",
    "\n",
    "Por otro lado, se puede vislumbrar que el modelo EfficientQDA mejora a los métodos precesores en términos de tiempo de entrenamiento, como así también se observa una mejora notable en los tiempos de predicción. Además, si bien consume más memoria que los métodos de QDA y TensorizedQDA, tiene una reducción importante comparado a FasterQDA. Estás mejoras en rendimiento general lo hace manteniendo lo mismos niveles de accuracy que logran los demás métodos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TP_AMIA",
   "language": "python",
   "name": "tp_amia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
